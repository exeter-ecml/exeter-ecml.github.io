---
layout: paper

title: "A Comprehensive Survey to Dataset Distillation"

authors:
- Shiye Lei
- Dacheng Tao

venue: "arXiv"

year: 2023

link: https://arxiv.org/abs/2301.05603

abstract: "
Deep learning technology has unprecedentedly developed in the last decade and
has become the primary choice in many application domains. This progress is
mainly attributed to a systematic collaboration that rapidly growing computing
resources encourage advanced algorithms to deal with massive data. However, it
gradually becomes challenging to cope with the unlimited growth of data with
limited computing power. To this end, diverse approaches are proposed to
improve data processing efficiency. Dataset distillation, one of the dataset
reduction methods, tackles the problem via synthesising a small typical dataset
from giant data and has attracted a lot of attention from the deep learning
community. Existing dataset distillation methods can be taxonomised into
meta-learning and data match framework according to whether explicitly mimic
target data. Albeit dataset distillation has shown a surprising performance in
compressing datasets, it still possesses several limitations such as distilling
high-resolution data. This paper provides a holistic understanding of dataset
distillation from multiple aspects, including distillation frameworks and
algorithms, disentangled dataset distillation, performance comparison, and
applications. Finally, we discuss challenges and promising directions to
further promote future studies about dataset distillation.
"

who_suggested: George De Ath
status: suggested
---
