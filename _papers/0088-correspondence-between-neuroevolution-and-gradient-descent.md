---
layout: paper

title: "Correspondence between neuroevolution and gradient descent"
authors:
- Stephen Whitelam
- Viktor Selin
- Sang-Won Park
- Isaac Tamblyn 
venue: Nature Communications
year: 2021

link: https://www.nature.com/articles/s41467-021-26568-2

abstract: "
We show analytically that training a neural network by conditioned stochastic
mutation or neuroevolution of its weights is equivalent, in the limit of small
mutations, to gradient descent on the loss function in the presence of Gaussian
white noise. Averaged over independent realizations of the learning process,
neuroevolution is equivalent to gradient descent on the loss function. We use
numerical simulation to show that this correspondence can be observed for
finite mutations, for shallow and deep neural networks. Our results provide a
connection between two families of neural-network training methods that are
usually considered to be fundamentally different.
"

who_suggested: George De Ath
status: suggested
---
- [supplementary material](https://static-content.springer.com/esm/art%3A10.1038%2Fs41467-021-26568-2/MediaObjects/41467_2021_26568_MOESM1_ESM.pdf)
