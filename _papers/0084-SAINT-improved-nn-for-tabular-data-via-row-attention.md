---
layout: paper

title: "SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training"
authors:
- Gowthami Somepalli
- Micah Goldblum
- Avi Schwarzschild
- C. Bayan Bruss
- Tom Goldstein
venue: Submitted to ICLR
year: 2021

link: https://openreview.net/forum?id=nL2lDlsrZU

abstract: "
Abstract: Tabular data underpins numerous high-impact applications of machine
learning from fraud detection to genomics and healthcare.  Classical approaches
to solving tabular problems, such as gradient boosting and random forests, are
widely used by practitioners. However, recent deep learning methods have
achieved a degree of performance competitive with popular techniques.  We
devise a hybrid deep learning approach to solving tabular data problems. Our
method, SAINT, performs attention over both rows and columns, and it includes
an enhanced embedding method. We also study a new contrastive self-supervised
pre-training method for use when labels are scarce. SAINT consistently improves
performance over previous deep learning methods, and it even performs
competitively with gradient boosting methods, including XGBoost, CatBoost, and
LightGBM, on average over  benchmark datasets in regression, binary
classification, and multi-class classification tasks.
"

who_suggested: George De Ath
status: suggested
---
- [github](https://github.com/somepago/saint)
- [tweets](https://papers.labml.ai/paper/ae31a2c0c41011eb80dc0bd1877e23b6)
